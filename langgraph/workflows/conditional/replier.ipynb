{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29dbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deaa9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMS = {}\n",
    "\n",
    "LLMS['gemini'] = ChatGoogleGenerativeAI(\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    model = 'gemini-1.5-flash'\n",
    ")\n",
    "\n",
    "LLMS['deepseek'] = ChatDeepSeek(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    api_base=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\"\n",
    ")\n",
    "\n",
    "LLMS['moonshotai'] = ChatGroq(\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\"),\n",
    "    model = \"moonshotai/kimi-k2-instruct\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165dbaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm: ChatGroq = LLMS['deepseek']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplyState(TypedDict):\n",
    "    review_text: str\n",
    "    sentiment: str\n",
    "    reply_text: str\n",
    "    issue_type: str\n",
    "    tone: str\n",
    "    urgency: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentSchema(BaseModel):\n",
    "    sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"Sentiment of the review text either positive or negative.\")\n",
    "    \n",
    "def determine_sentiment(state: ReplyState):\n",
    "    review_text = state['review_text']\n",
    "    structered_llm1 = llm.with_structured_output(SentimentSchema)\n",
    "    sentiment_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        You are an expert at understanding human language and emotion. Your task is to read a review and decide whether the overall sentiment is Positive or Negative.\n",
    "\n",
    "        Focus on the review's tone, intent, and emotional impact—not just isolated words. If the review contains mixed signals, choose the side that clearly dominates. Be confident and consistent.\n",
    "\n",
    "        Return your answer in exactly this format in lower case:\n",
    "        sentiment: positive\n",
    "        or\n",
    "        sentiment: negative\n",
    "\n",
    "        Don't add explanations, confidence scores, or extra text. Just the sentiment.\n",
    "        \"\"\"),\n",
    "        (\"human\", \"Tell me the sentiment of this review text: {review_text}\")\n",
    "    ])\n",
    "    chain1 = sentiment_prompt | structered_llm1\n",
    "    result1 = chain1.invoke(input={\n",
    "        'review_text': review_text\n",
    "    })\n",
    "    \n",
    "    return {'sentiment': result1.sentiment}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosisSchema(BaseModel):\n",
    "    issue_type: str = Field(description=\"The type of a issue in the review text.\")\n",
    "    tone: str = Field(description=\"The tone of a user.\")\n",
    "    urgency: str = Field(description=\"The urgency level according to the review text.\")\n",
    "    \n",
    "def run_diagnosis(state: ReplyState) -> ReplyState:\n",
    "    review_text = state['review_text']\n",
    "    structered_model2 = llm.with_structured_output(DiagnosisSchema);\n",
    "    diagnosis_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        You are a detailed reviewer assistant, responsible for analyzing negative reviews and providing insights on three key areas:\n",
    "\n",
    "        Issue Type: Identify the main problem or complaint the user is expressing. This could relate to the product's functionality, quality, customer service, shipping, or any other category of dissatisfaction.\n",
    "\n",
    "        Tone: Determine the overall emotional tone of the review. Is the tone angry, frustrated, disappointed, regretful, sad, or neutral (despite the negativity)? This reflects how strongly the reviewer feels about the issue.\n",
    "\n",
    "        Urgency: Assess how urgent the reviewer's concern seems. Is this an issue the reviewer feels needs immediate attention, or is it something that could be resolved over time? Classify the urgency as High, Medium, or Low based on the intensity of the complaint and how urgently the reviewer asks for resolution.\n",
    "        \n",
    "        # Example\n",
    "        Input - I ordered this laptop two weeks ago, and it still hasn't shipped. I've called customer service three times, and they just keep giving me the runaround. I need my laptop now!\n",
    "        Output -\n",
    "        issue_type: shipping delay\n",
    "        tone: frustrated\n",
    "        urgency: high\n",
    "        \"\"\"),\n",
    "        (\"human\", \"Diagnosis this review: {review_text}\")\n",
    "    ])\n",
    "    chain2 = diagnosis_prompt | structered_model2\n",
    "    result2 = chain2.invoke(input={\n",
    "        'review_text': review_text\n",
    "    })\n",
    "    \n",
    "    return {'issue_type': result2.issue_type, 'tone': result2.tone, 'urgency': result2.urgency}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90835ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplySchema(BaseModel):\n",
    "    reply: str = Field(description=\"The reply of a review, according to the review.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_response(state: ReplyState):\n",
    "    review_text = state['review_text']\n",
    "    structered_llm3 = llm.with_structured_output(ReplySchema);\n",
    "    negative_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        You are a customer support assistant responding to a negative review. Your task is to craft a reply based on the following parameters:\n",
    "        \n",
    "        - Issue Type: Identify the main problem or complaint mentioned in the review. Whether it's related to the product, service, delivery, or any other concern, address the core issue directly and empathetically.\n",
    "        - Tone: Match the tone of your response to the tone of the review. If the review is angry or frustrated, your response should be apologetic and empathetic. If the tone is more neutral or regretful, your reply should be understanding but professional.\n",
    "        - Urgency: If the reviewer indicates a sense of urgency (e.g., a shipping delay or a defective product), prioritize your response to assure them that their concern will be addressed quickly. If the urgency is low, your response can be more general but still respectful.\n",
    "\n",
    "        Reply Guidelines:\n",
    "        - Acknowledge the issue and empathize with the customer's experience.\n",
    "        - Apologize for any inconvenience caused.\n",
    "        - Provide a clear course of action, if applicable (e.g., refund, replacement, or support).\n",
    "        - Offer a solution or next steps based on the issue type and urgency.\n",
    "        - Maintain a friendly, professional tone throughout the response.\n",
    "        \n",
    "        # Output Format\n",
    "        Output must only have a respond text, no explainations, no extra lines, nothing else.\n",
    "        \"\"\"),\n",
    "        (\"human\", \"\"\"\n",
    "        Give a appropriate reply to this review: {review_text}.\n",
    "        ---\n",
    "        Here are the diagnosis of the above review: \n",
    "        Issue Type - {issue_type}\n",
    "        Tone - {tone}\n",
    "        Urgency - {urgency}\n",
    "        \"\"\")\n",
    "    ])\n",
    "    chain3 = negative_prompt | structered_llm3\n",
    "    result3 = chain3.invoke(input={\n",
    "        'review_text': review_text,\n",
    "        'issue_type': state['issue_type'],\n",
    "        'tone': state['tone'],\n",
    "        'urgency': state['urgency']\n",
    "    })\n",
    "    \n",
    "    return {'reply_text': result3.reply}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d2cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_response(state: ReplyState):\n",
    "    review_text = state['review_text']\n",
    "    structered_llm4 = llm.with_structured_output(ReplySchema);\n",
    "    positive_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        You are a customer support assistant responding to a positive review. Your goal is to craft a warm, appreciative, and professional response based on the review content.\n",
    "        - Tone: Reflect the tone of the review in your reply. If the review is enthusiastic, excited, or grateful, mirror that energy in your response. If the review is more neutral but positive, maintain a friendly and appreciative tone.\n",
    "        - Acknowledgment: Express gratitude for the positive feedback. Let the reviewer know you appreciate their time in leaving the review and their support for your product or service.\n",
    "        - Personalization: Try to personalize your reply by referring to specific elements of the review, whether it's a particular feature they liked, an experience they had, or a positive outcome they mentioned.\n",
    "        - Future Engagement: Encourage future engagement, whether it's another purchase, reaching out for support, or sharing their experience with others. Offer a continued positive relationship.\n",
    "\n",
    "        Reply Guidelines:\n",
    "        - Show appreciation and gratitude.\n",
    "        - Acknowledge any specific details mentioned in the review (e.g., a favorite feature, great customer service).\n",
    "        - Keep the tone positive and welcoming.\n",
    "        - End with an invitation for further engagement or contact.\n",
    "        \"\"\"),\n",
    "        (\"human\", \"Give an appropriate reply to this review: {review_text}\")\n",
    "    ])\n",
    "    chain4 = positive_prompt | structered_llm4\n",
    "    result4 = chain4.invoke(input={\n",
    "        'review_text': review_text\n",
    "    })\n",
    "    \n",
    "    return {'reply_text': result4.reply};\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_condition(state: ReplyState) -> Literal[\"positive_response\", \"run_diagnosis\"]:\n",
    "    if state[\"sentiment\"] == \"positive\":\n",
    "        return \"positive_response\"\n",
    "    else:\n",
    "        return \"run_diagnosis\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(ReplyState)\n",
    "\n",
    "graph.add_node('determine_sentiment', determine_sentiment)\n",
    "graph.add_node(\"run_diagnosis\", run_diagnosis)\n",
    "graph.add_node(\"negative_response\", negative_response)\n",
    "graph.add_node(\"positive_response\", positive_response)\n",
    "\n",
    "graph.add_edge(START, \"determine_sentiment\")\n",
    "graph.add_conditional_edges(\"determine_sentiment\", check_condition)\n",
    "graph.add_edge(\"run_diagnosis\", \"negative_response\")\n",
    "graph.add_edge(\"negative_response\", END)\n",
    "graph.add_edge(\"positive_response\", END)\n",
    "\n",
    "workflow = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    'review_text': \"I've been using this blender for a few weeks now, and it's amazing! It blends everything smoothly, even frozen fruit. The cleanup is super easy too. Best purchase I've made this year!\"\n",
    "}\n",
    "\n",
    "final_state = workflow.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e67af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_text': \"I've been using this blender for a few weeks now, and it's amazing! It blends everything smoothly, even frozen fruit. The cleanup is super easy too. Best purchase I've made this year!\",\n",
       " 'sentiment': 'positive',\n",
       " 'reply_text': \"Thank you so much for your enthusiastic review! We're thrilled to hear that you're loving your blender and that it's performing so well, especially with frozen fruit and easy cleanup. It’s fantastic to know it’s one of your best purchases this year—that’s exactly the kind of experience we aim to provide. If you ever have any questions or need tips, don’t hesitate to reach out. Happy blending!\"}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59259bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    'review_text': \"I ordered a jacket that was supposed to fit like a medium, but it was way too small. I followed the size chart exactly. Very disappointed, and now I have to deal with the return process. Not happy!\"\n",
    "}\n",
    "\n",
    "final_state = workflow.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af83b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_text': 'I ordered a jacket that was supposed to fit like a medium, but it was way too small. I followed the size chart exactly. Very disappointed, and now I have to deal with the return process. Not happy!',\n",
       " 'sentiment': 'negative',\n",
       " 'reply_text': 'We’re so sorry to hear that the jacket didn’t fit as expected, despite following the size chart—this must be frustrating. We appreciate you bringing this to our attention, and we’d love to make it right for you. Please reach out to our customer service team at [support email/phone], and we’ll assist you with the return process and ensure you receive a refund or replacement as quickly as possible. We value your feedback and will use it to improve our sizing guidance. Thank you for your patience, and we apologize for the inconvenience.',\n",
       " 'issue_type': 'incorrect sizing',\n",
       " 'tone': 'disappointed',\n",
       " 'urgency': 'medium'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
